til 2주차

1. 배운것


*supervised learning


* bagging, boosting


* HW 2: Make a better model for TADA ETA data

다양한 모델(결정나무, gradient boosting 등)을을 학습시켜서 테스트 셋의 값을 예측하고 정확도를 비교해보는 것을 배웠다

2. 느낀점

이번 강의는 이번학기에 학교에서 배우고 있는 데이터 사이언스 수업에서 배운 부분들과 거의 비슷하였다. 수업에서 결정나무로 pre프루닝, post프루닝등의 과정을 거쳐서 더 나은 모델을 만드는
실습을 했었기에 이번 과제에 이용해 보려고 했으나, 목표변수가 명목형이어서 이번 과제에는 적용할 수 없었다.
gradient boosting기법을 사용하였고, 각 파라미터들을 1부터 넣고 최적을 파라미터 값을 찾는 방식으로 과제를 수행하였다.
그러나 mse의 경우 최적의 파라미터들만 조합하였음에도 불구하고 오히려 기본 모델보다 mse오차 값이 커져버렸다.
물론 상식적으로 모델의 크기, split 기준등 각 파라미터들 간의 조합이 중요하다는것은 알고 있었으나 과거 학교 수업의 실습에서는 같은 방식에도 오차가 크게 개선되어
이번에도 유효할 것 이라고 생각했던 잘못이었다.
따라서 mse는 수작업을 통해 경험적으로 가장 낮은 수치를 만들어야 했다.
