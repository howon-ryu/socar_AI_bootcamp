[8-2] 
딥러닝의 경우 학습에 많은 시간과 비용이 필요

그래서 fine-tuning사용(현재의 문제가 기존 학습 모델과 유사할때 사용)
그래서 이미 학습된 모델 parameter를 initial 로 해서 현재 문제에 맞게 추가학습을 하는것!
=>작은 learning rate(1/10)를 사용하여 기존 parameter를 변화시키지 않도록 함

fine-tuning의 적용 방법
1. 모델 전체 업데이트
2. 앞부분은 고정(freeze),뒷부분만 업데이트

ensemble-> 모델들의 예측을 합쳐서 강력한 모델을 생성
합치는 방법 :
hard voting(더 확률이 높은것 선택), soft voting(더한후 나누기), weighted voting(가중치 방식)

모델의 종류 : alexnet, vgg, resnet

[8-3]
self-supervised learning (자기지도 학습)
배경 : 딥러닝은 지도 학습을 했을때 성능이 굉장히 좋음
하지만 supervised learning(지도 학습)은 하고싶다고 할수 있는것이 아님!(정답 label을 주어야하기 때문에)
그렇다면 어떻게 label이 없는데도 지도학습처럼 학습을 할수 있을까?
=> self-supervised learning

과정
1.사용자가 직접 만든 task인 pretext task를 정의
2. label이 없는 데이터를 변형하여 pretext task를 학습할 수 있는 supervision을 생성
3. 생성한 supervision으로 pretask에 대해 학습

autoencoder에서 supervision은 원본이미지에 해당

self-supervised leaning을 통해 학습시킨 모델을 pretrained network 로 사용하여 downstream task(실제로 목표하는 작업)의 기반으로 확용

그렇다면 self-supervised leaning을 통해 학습시킨 모델이 진짜 학습이 잘됬는지는 어떻게 알까?
=> label이 있는 데이터 셋에 자기지도 학습을 하여 확인

좋은 feature extractor를 뽑았는지 확인하는 방법은?
=> self-supervised로 pretrain된 부분을 freeze한 채로 label이 있는 데이터에 대해 학습하여 성능을 측정
